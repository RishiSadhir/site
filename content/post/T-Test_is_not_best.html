---
title: "T-Test is not BEST"
author: "Rishi Sadhir"
date: '2019-12-31'
categories: ["R", "Bayesian", "Statistics"]
tags: ["R", "Bayesian", "Statistics"]
---



<p>In this post, we explore the task of comparing groups of measurements. We’ve all heard about the T-Test, but Bayesian statistics offers an interesting alternative. Let’s talk about how “Bayesian estimation supersedes the t-test” (<a href="https://cran.r-project.org/web/packages/BEST/vignettes/BEST.pdf">Kruschke, 2013</a>).</p>
<div id="preliminaries" class="section level1">
<h1>Preliminaries</h1>
<p>In this post, we’ll encode Kruschke’s methods in STAN and use it to compare action move ratings versus comedy movie ratings. The code shown below is also available in python <a href="https://github.com/RishiSadhir/T-Test-is-not-BEST/blob/master/T-Test%20is%20not%20best.ipynb">here</a> along with the supporting CSV. We make liberal use of the tidyverse grouping of R packages to carry out our analysis. Those unfamiliar can reference the excellent <a href="https://r4ds.had.co.nz/">R4DS</a> book by Hadley Wickham.</p>
<pre class="r"><code>library(tidyverse)
library(patchwork) # Stitch together ggplots
library(viridis) # Be colorblind friendly 
library(rstan) # Make calls to STAN
library(tidybayes) # Work with bayesian posteriors

options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)

theme_minimal() %+replace%
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.background = element_rect(color = &quot;transparent&quot;, fill = &quot;grey90&quot;)) %&gt;% 
  theme_set()</code></pre>
</div>
<div id="comedy-or-action" class="section level1">
<h1>Comedy or Action?</h1>
<p>Lets load our data set from the <code>ggplot2movies</code> package. This dataset contains 28819 movies from IMDB. We’ll use it to compare the rating of action movies vs. comedy movies.</p>
<pre class="r"><code>library(ggplot2movies)

# Clean up data
set.seed(1234)  # Set seed so we get the same sampled rows every time
movies_clean &lt;- movies %&gt;% 
  select(title, year, rating, Action, Comedy) %&gt;% 
  filter(!(Action == 1 &amp; Comedy == 1)) %&gt;% 
  mutate(genre = case_when(Action == 1 ~ &quot;Action&quot;,
                           Comedy == 1 ~ &quot;Comedy&quot;,
                           TRUE ~ &quot;Neither&quot;)) %&gt;%
  filter(genre != &quot;Neither&quot;) %&gt;%
  group_by(genre) %&gt;% 
  sample_n(200) %&gt;% 
  ungroup() %&gt;% 
  select(-Action, -Comedy)

head(movies_clean)</code></pre>
<pre><code>## # A tibble: 6 x 4
##   title                year rating genre 
##   &lt;chr&gt;               &lt;int&gt;  &lt;dbl&gt; &lt;chr&gt; 
## 1 Tarzan Finds a Son!  1939    6.4 Action
## 2 Silmido              2003    7.1 Action
## 3 Stagecoach           1939    8   Action
## 4 Diamondbacks         1998    1.9 Action
## 5 Chaos Factor, The    2000    4.5 Action
## 6 Secret Command       1944    7   Action</code></pre>
<p>We are interested in analyzing the difference in these two groups. Are they statistically significantly different or is it just noise?</p>
<pre class="r"><code>ggplot(movies_clean, aes(x = rating, y = fct_rev(genre), fill = genre)) +
  geom_halfeyeh() +
  theme(panel.grid.major.x = element_line(color = &quot;white&quot;),
        legend.position = &quot;none&quot;) +
  scale_x_continuous(breaks = 0:10) +
  scale_fill_manual(values = viridis::inferno(5, alpha = .6)[c(3,4)]) +
  ylab(&quot;&quot;) + xlab(&quot;Rating&quot;) + ggtitle(&quot;Observed ratings by genre&quot;)</code></pre>
<p><img src="/post/T-Test_is_not_best_files/figure-html/unnamed-chunk-3-1.png" width="480" /></p>
<p>Although they look very similar visually, a t-test tells us that they are in fact statistically significantly different with 95% confidence.</p>
<pre class="r"><code>t.test(filter(movies_clean, genre == &quot;Comedy&quot;)$rating,
       filter(movies_clean, genre == &quot;Action&quot;)$rating) %&gt;% 
  print</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  filter(movies_clean, genre == &quot;Comedy&quot;)$rating and filter(movies_clean, genre == &quot;Action&quot;)$rating
## t = 2.8992, df = 388.75, p-value = 0.003953
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.1400087 0.7299913
## sample estimates:
## mean of x mean of y 
##     5.842     5.407</code></pre>
</div>
<div id="best" class="section level1">
<h1>BEST</h1>
<p><a href="https://cran.r-project.org/web/packages/BEST/vignettes/BEST.pdf">Kruschke 2013</a> provides well-reasoned arguments for favoring Bayesian parameter estimation over null hypothesis significance testing to compare two groups. It also introduced a robust model for comparing two groups, which modeled the data as t-distributed, instead of a Gaussian distribution.</p>
<blockquote>
<p>“When data are interpreted in terms of meaningful parameters in a mathematical description, such as the difference of mean parameters in two groups, it is Bayesian analysis that provides complete information about the credible parameter values. Bayesian analysis is also more intuitive than traditional methods of null hypothesis significance testing…”</p>
</blockquote>
<p>So what does this alternative look like? We describe it in the model below. Wait a second… Why run a model when you can just run a T-test? At the end of the day, a T-test is itself a model with assumptions of its own. What we are doing here isn’t all that complicated, we’re just obviating all the assumptions in our method. A key thing to notice - We are free to look further than just the first moment of our observed distrbutions. STAN lets us flexibly model the mean, standard deviation, and degrees of freedom separately.</p>
<p><span class="math display">\[
\begin{align}
rating &amp;\sim student\_t(\nu, \mu_i, \sigma_i) \\
\mu_i &amp;= \alpha_{group[i]} \\
\sigma_i &amp;= \gamma_{group[i]} \\
\nu &amp;\sim cauchy(0, 1) \\
\alpha &amp;\sim normal(5.5, 2) \\
\sigma &amp;\sim cauchy(0, 1) \\
\end{align}
\]</span></p>
<p>All we are saying here is that ratings are normally distribted and their location and spread depend on whether or not the movie is a comedy or an action flick. The last three lines are priors. Priors are a bayesian concept that lets you encode prior information you have in to your model. In this case, we ask our model to be a bit skeptical of group differences by assuming the overall mean and standard deviation to be true for all sub groups. This is effectively a very weak form of regularization… but more on that in another post.</p>
<p>We encode this math in STAN below. If you aren’t use to seeing stan code, I’ll provide a very high level overview of what you’re looking at. The first <code>data</code> block describes the what we are going to pass in from R. Note that we include both scalars and vectors here. <code>transformed data</code> does some light preprocessing to create sane priors. The <code>parameters</code> section outline what STAN is search over - we have an intercept for each groups mean and an intercept for each groups standard deviation. The <code>model</code> section is where we encode the models log-likelihood. It tells STAN how good a candidate set of parameter values is at describing the data.</p>
<pre class="stan"><code>data {
  int&lt;lower=1&gt; N;                           // Sample size
  int&lt;lower=2&gt; n_groups;                    // Number of groups
  vector[N] rating;                         // Outcome variable
  int&lt;lower=1, upper=n_groups&gt; group_id[N]; // Group variable
}
transformed data {
  real mean_rating;
  real sd_rating;
  mean_rating = mean(rating);
  sd_rating = sd(rating);
}
parameters {
  vector&lt;lower=0,upper=10&gt;[n_groups] alpha; // Group means 
  vector&lt;lower=0&gt;[n_groups] gamma;          // Group sds
  real&lt;lower=0, upper=100&gt; nu;              // df for t distribution
}
model {
  real location;
  real scale;
  
  alpha ~ normal(mean_rating, sd_rating);
  gamma ~ cauchy(0, 1);
  nu ~ cauchy(0, 1);
  
  for (n in 1:N){
    location = alpha[group_id[n]];
    scale = gamma[group_id[n]];
    rating[n] ~ student_t(nu, location, scale);
  }
}
generated quantities {
  // Mean difference
  real mu_diff;
  real sigma_diff;
  mu_diff = alpha[1] - alpha[2];
  sigma_diff = gamma[1] - gamma[2];
}</code></pre>
<p>Now then… Lets set up our data to comply with the <code>data</code> block above and pass it over to STAN to perform MCMC sampling. We set up our index variables such that group_id 1 maps to comedy and group_id 2 maps to action.</p>
<pre class="r"><code>dlist &lt;- list(
  N = nrow(movies_clean),
  n_groups = length(unique(movies_clean$genre)),
  rating = movies_clean$rating,
  group_id = as.integer(fct_rev(movies_clean$genre)))

fit &lt;- sampling(stan_best, data = dlist, 
                chains = 4, cores = 4, refresh = 0)
print(fit, probs = c(.05, .95))</code></pre>
<pre><code>## Inference for Stan model: 4303c9d63e697571bc859d7ca25389fc.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##               mean se_mean    sd      5%     95% n_eff Rhat
## alpha[1]      5.87    0.00  0.10    5.70    6.04  3135    1
## alpha[2]      5.41    0.00  0.11    5.23    5.60  3790    1
## gamma[1]      1.32    0.00  0.08    1.20    1.45  3012    1
## gamma[2]      1.57    0.00  0.09    1.44    1.72  2944    1
## nu           35.35    0.53 22.42   11.00   83.56  1767    1
## mu_diff       0.46    0.00  0.15    0.21    0.72  3684    1
## sigma_diff   -0.25    0.00  0.11   -0.43   -0.07  3624    1
## lp__       -504.10    0.04  1.61 -507.16 -502.10  1718    1
## 
## Samples were drawn using NUTS(diag_e) at Sun Feb  9 16:36:55 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>MCMC sampling is really cool. It basically “randomly” explores the parameter space in proportion to the how likely sets of parameters are. This exploration is graphed below for the intercepts in our model.</p>
<pre class="r"><code>post &lt;- fit %&gt;% 
  gather_draws(`(alpha.*|gamma.*)`, regex = TRUE) %&gt;%
  mutate(chain = as.factor(.chain))

p1 &lt;- post %&gt;% 
  ggplot(aes(x = .iteration, y = .value, color = chain)) +
    geom_line(alpha = .3) +
    facet_wrap(~ .variable, scales = &quot;free&quot;) +
    scale_color_viridis_d() +
    labs(x = &quot;iteration&quot;, y = &quot;value&quot;,
         title = &quot;Four chains explore the parameter space\nin parallel&quot;) +
    theme(legend.position = &quot;none&quot;,
          panel.grid.major.y = element_line(color = &quot;white&quot;))

p2 &lt;- post %&gt;% 
  mutate(.chain = as.factor(.chain)) %&gt;% 
  group_by(.variable, .chain) %&gt;% 
  mutate(.value = cummean(.value)) %&gt;% 
  ggplot(aes(x = .iteration, y = .value, color = chain)) +
    geom_line(alpha = .5) +
    facet_wrap(~ .variable, scales = &quot;free&quot;) +
    scale_color_viridis_d() +
    labs(x = &quot;iteration&quot;, y = &quot;cumulative mean&quot;,
         title = &quot;Parameter exploration converges to the mean\nacross markov chains&quot;) +
    theme(panel.grid.major.y = element_line(color = &quot;white&quot;))

p1 + p2 + plot_layout(guides = &#39;collect&#39;)</code></pre>
<p><img src="/post/T-Test_is_not_best_files/figure-html/unnamed-chunk-7-1.png" width="960" /></p>
</div>
<div id="analysis" class="section level1">
<h1>Analysis</h1>
<p>Now that we ran (and believe) our model, lets examine its personality. Our Bayesian method gives us a robust understanding of the data generating process. Below, we take our posterior samples and examine their values. Like I mentioned earlier, our samples are returned in proportion to their likelihood (and prior information). We interpret the x-axis in the graphs below as the probability of that particular value being the true value of the parameter given what we’ve seen in our data and our priors: <span class="math inline">\(Pr(\theta | Data, Priors)\)</span>.</p>
<pre class="r"><code>plot_posterior_moment &lt;- function(vec, width = .95) {
  m &lt;- mean(vec)
  hdi &lt;- as.vector(hdi(vec, .width = width))
  df &lt;- enframe(vec)
  
  ggplot(df, aes(vec)) +
    stat_density(geom = &quot;line&quot;, size = 1, color = viridis(1)) +
    geom_segment(aes(x = hdi[1], xend = hdi[2], 
                     y = 0, yend = 0),
                 color = viridis(1)) +
    geom_point(aes(y=0, x = m), size = 2, shape = 1, color = viridis(1)) +
    scale_y_continuous(NULL, NULL) + xlab(&quot;Posterior Distribution&quot;) +
    theme(panel.grid.major.y = element_blank(),
          panel.grid.major.x = element_line(color = &quot;white&quot;))
}

post &lt;- gather_draws(fit, alpha[group], gamma[group]) %&gt;% 
  ungroup() %&gt;% 
  mutate(group = str_c(&quot;Group &quot;, group))

p1 &lt;- post %&gt;% 
  filter(.variable == &quot;alpha&quot;, group == &quot;Group 1&quot;) %&gt;% 
  pull(.value) %&gt;% 
  plot_posterior_moment() +
    scale_x_continuous(name = NULL, breaks = seq(4.9, 6.3, .1), limits = c(4.9, 6.3)) +
    labs(title = expression(mu[comedy]))
p2 &lt;- post %&gt;% 
  filter(.variable == &quot;alpha&quot;, group == &quot;Group 2&quot;) %&gt;% 
  pull(.value) %&gt;% 
  plot_posterior_moment() +
    scale_x_continuous(name = NULL, breaks = seq(4.9, 6.3, .1), limits = c(4.9, 6.3)) +
    labs(title = expression(mu[action]))
p3 &lt;- post %&gt;% 
  filter(.variable == &quot;gamma&quot;, group == &quot;Group 1&quot;) %&gt;% 
  pull(.value) %&gt;% 
  plot_posterior_moment() +
  scale_x_continuous(name = NULL, breaks = seq(1, 2, by = .1), limits = c(1,2)) +
    labs(title = expression(sigma[comedy]))
p4 &lt;- post %&gt;% 
  filter(.variable == &quot;gamma&quot;, group == &quot;Group 2&quot;) %&gt;% 
  pull(.value) %&gt;% 
  plot_posterior_moment() +
  scale_x_continuous(name = NULL, breaks = seq(1, 2, by = .1), limits = c(1,2)) +
    labs(title = expression(sigma[action]))

p1 + p3 + p2 + p4 +
  plot_annotation(title = &quot;Posterior Moments&quot;,
                  subtitle = &quot;Shown with 95% credibility about the mean&quot;)</code></pre>
<p><img src="/post/T-Test_is_not_best_files/figure-html/unnamed-chunk-8-1.png" width="960" /></p>
<p>We can do fun things with our posterior distributions. In particular, we can calculate the difference between means (also known as a contrast) between two groups and formally analyze the probability of comedy movies getting higher ratings than action movies.</p>
<pre class="r"><code>post_mudiff &lt;- spread_draws(fit, mu_diff, sigma_diff)
p1 &lt;- plot_posterior_moment(post_mudiff$mu_diff) +
  scale_x_continuous(expression(mu[comedy] - mu[action]), breaks = seq(-.1, 1.1, by = .1), limits = c(-.1, 1.1)) +
  labs(title = &quot;Posterior difference in means&quot;,
       subtitle = &quot;Shown with 95% credibility about the mean&quot;)

p2 &lt;- plot_posterior_moment(post_mudiff$sigma_diff) +
  scale_x_continuous(expression(sigma[comedy] - sigma[action]), breaks = seq(-.7, 1, .1), limits = c(-.7, .1), labels = round(seq(-.7, 1, .1), 1)) +
  labs(title = &quot;Posterior difference in standard deviations&quot;,
       subtitle = &quot;Shown with 95% credibility about the mean&quot;)


p1 + p2</code></pre>
<p><img src="/post/T-Test_is_not_best_files/figure-html/unnamed-chunk-9-1.png" width="960" /></p>
<p>We can use this difference to directly calculate the probability of the mean of comedy ratings being higher than the mean of action ratings to be 99.85%. We also calculate the 95 percent confidence interval of the difference to be between [.16, .75].</p>
<pre class="r"><code>get_mode &lt;- function(v) {
   uniq &lt;- unique(v)
   uniq[which.max(tabulate(match(v, uniq)))]
}

p_val &lt;- with(post_mudiff, mean(mu_diff &gt; 0))
diff_mode  &lt;- with(post_mudiff, get_mode(mu_diff))
hdi &lt;- HDInterval::hdi(post_mudiff$mu_diff)</code></pre>
<p>We are 99.85% sure comedy movies rate higher than action movies. We are 95% sure that difference is between 0.17 and 0.77. Finally, the most likely value for the difference is 0.58. Note that this is in contrast to our interpretation of the results of the t-test which claims that 95% of alternative universes with alternative movies would have comedy ratings higher than action ratings by [.14, .72] on average.</p>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Its important to not be dogmatic about your tooling. In our case, the frequentist approach lined up pretty well with the bayesian approach… and was way easier to reach for! However, its equally important to remember the that these quick procedures come with a lot of assumptions - for example our t-test was run with a tacit equal variance assumption which can affect the Type I error rate when violated. Bayesian approaches, though more verbose, force you to spell out the exact model you are using to explain your data… and there is always a model under the hood, even for something as simple as a t-test.</p>
</div>

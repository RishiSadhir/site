---
title: "Catch and Release"
date: '2020-03-20'
output: html_notebook
---



<p>Today we’re going to think through how to estimate the number of fish in a lake. This kind of problem is faced regularly by <a href="https://en.wikipedia.org/wiki/Mark_and_recapture">ecologists</a> trying to estimate population sizes. It will also give us a great tour of some cool STAN fundamentals. Lets get started.</p>
<pre class="r"><code>library(tidyverse)
library(rstan)</code></pre>
<p>Our approach will be to tag a few fish, wait a while, then catch some more. The proportion of new fish with original tags will give us the information we need to make estimates about the larger population.</p>
<p>To make it concrete, lets simulate some data. Suppose we catch, tag, and release 68 fish. Then, after a month, we go fishing again. This time we landed 219 rainbow trout, 16 of which were marked.</p>
<pre class="r"><code># Data collection
dlist &lt;- list(
  tagged = 68,
  captured = 219,
  returned = 16)
dlist</code></pre>
<pre><code>## $tagged
## [1] 68
## 
## $captured
## [1] 219
## 
## $returned
## [1] 16</code></pre>
<p>To buid our intuition for this problem, we can think of the ratio of marked fish in our recapture session as representative of the larger population.</p>
<p><span class="math display">\[
\begin{align}
\frac{returned}{captured} &amp;= \frac{tagged}{N} \\
N &amp;= \frac{tagged}{\frac{returned}{captured}}
\end{align}
\]</span></p>
<p>Popping this equation in to R and getting our estimate is easy.</p>
<pre class="r"><code>map &lt;- with(dlist, tagged / (returned / captured))
map</code></pre>
<pre><code>## [1] 930.75</code></pre>
<p>The question becomes more interesting when you consider the amount of uncertainty around that estimate. This is important to quantify because it may influence us to gather more data by going out and tagging catching, and releasing again.</p>
<p>To get that more holistic answer to this question, lets start model building. We’ll consider each caught fish as a bernoulli trial where a success is whether or not it was tagged. We can aggregate up bernoulli trials into a binomial model. This is shown in the equation below. Our free parameter is the number of total fish in the lake, <span class="math inline">\(N\)</span>.</p>
<p><span class="math display">\[
\begin{align}
returned &amp;\sim Binomial(captured, \frac{tagged}{N}) \\
\end{align}
\]</span></p>
<p>Lets encode this model directly in stan. Note that we are using a flat prior which will make the posterior representative of the MAP estimate above. In reality, we would probably have decent priors based on the size of the lake, previous studies, or information about other lakes.</p>
<pre class="stan"><code>data {
  int&lt;lower=0&gt; tagged;
  int&lt;lower=0&gt; captured;
  int&lt;lower=0, upper=min(tagged, captured)&gt; returned;
}
parameters {
  real&lt;lower=(captured + tagged - returned)&gt; N;
}
model {
  returned ~ binomial(captured, tagged / N);
}</code></pre>
<p>Lets think about our free parameter, <span class="math inline">\(N\)</span>, for a second. We have some information on how to set a lower bound for this parameter in the problem setup. Our first tagging session has us seeing 68 fish. The second capture session netted us 271 fish. Between those two sets, there were 16 fish in common. So we know that there are at least <span class="math inline">\(219 + 68 - 16 = 271\)</span> fish out there. It’s actually important to get this lower bound right. Our sampler will have trouble with its binomial model if it starts trying to grab samples larger than the population.</p>
<pre class="stan"><code>data {
  int&lt;lower=0&gt; tagged;
  int&lt;lower=0&gt; captured;
  int&lt;lower=0, upper=min(tagged, captured)&gt; returned;
}
parameters {
  real N;
}
model {
  returned ~ binomial(captured, tagged / N);
}</code></pre>
<p>Now that we’ve spelled it all out, let’s take the model to the data. We pop our observations in to stan and let it search around for N.</p>
<pre class="r"><code>fit &lt;- sampling(lincolnpeterson, data = dlist, refresh = 0,
                chains = 4, cores = 4, iter = 4000)
print(fit, probs = c(.95, .05))</code></pre>
<pre><code>## Inference for Stan model: 96327ec92b20392ff877007fc2dc4d1a.
## 4 chains, each with iter=4000; warmup=2000; thin=1; 
## post-warmup draws per chain=2000, total post-warmup draws=8000.
## 
##         mean se_mean     sd     95%     5% n_eff Rhat
## N    1059.55    5.35 282.83 1587.69 682.89  2796    1
## lp__  -51.23    0.01   0.72  -50.72 -52.75  2988    1
## 
## Samples were drawn using NUTS(diag_e) at Sun Mar 22 22:33:29 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>Lets zoom in to the N parameter and visualize it. I’m always playing with different ways to plot posterior distributions and this is my latest favorite. It’s interesting to note the difference between mode and mean of the distribution. Our analysis has allocated significant belief to the right of the distribution. This goes to show that the MAP values returned by frequentist analyses aren’t always the best to go with.</p>
<pre class="r"><code>posterior &lt;- rstan::extract(fit)
dens &lt;- density(posterior$N)

df_dens &lt;- tibble(
  x = dens$x,
  y = dens$y)
  
ci &lt;- round(as_vector(HDInterval::hdi(posterior$N)), 1)
m &lt;- round(mean(posterior$N), 1)

df_dens %&gt;% 
  ggplot(aes(x, y)) +
    geom_vline(xintercept = map, linetype=2) +
    geom_vline(xintercept = m) +
    geom_line() +
    geom_ribbon(data = filter(df_dens, x &gt; ci[[1]] &amp; x &lt; ci[[2]]),
                aes(ymin = 0, ymax = y), 
                fill = &quot;grey&quot;, color = &quot;transparent&quot;) +
    annotate(&quot;text&quot;, x = 975, y = .0002, label = glue::glue(&quot;95% Credibe Interval\n[{ci[1]}, {ci[2]}]&quot;)) +
    annotate(&quot;text&quot;, x = 975, y = .0012, label = glue::glue(&quot;Mean: {m}&quot;)) +
    ggtitle(&quot;Bayesian estimation of fish population&quot;, &quot;Results from a Lincoln-Peterson simulation&quot;) +
    labs(caption = &quot;The distribution is overlayed on top of the MAP estimate calculated earlier (dotted line) and the mean (solid line)&quot;) +
    scale_y_continuous(&quot;Density&quot;) +
    scale_x_continuous(&quot;Posterior fish population size&quot;, breaks = as.numeric(c(ci, m, map))) +
    theme(panel.background = element_rect(color = &quot;black&quot;, fill = &quot;transparent&quot;),
          panel.grid = element_blank())</code></pre>
<p><img src="/post/catch_and_release_files/figure-html/unnamed-chunk-7-1.png" width="1152" /></p>
<p>And there we have it. With this distribution we can make future informed decisions based on the health of the lake. We can also gather more data to get more certainty about our measurment. We made a major assumption with this model - there were no deaths or births in the larger population. There are some really cool new <a href="http://www.stats.otago.ac.nz/webdata/resources/matthew_schofield/PhD_research/PhDThesisrevised.pdf">higherarchical flavors of mark and recapture models</a> that I may revisit here if there is interest.</p>

<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Rishi Sadhir&#39;s personal blog</title>
    <link>/post/</link>
    <description>Recent content in Posts on Rishi Sadhir&#39;s personal blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Catch and Release</title>
      <link>/2020/03/20/catch-and-release/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/20/catch-and-release/</guid>
      <description>Today we’re going to think through how to estimate the number of fish in a lake. This kind of problem is faced regularly by ecologists trying to estimate population sizes. It will also give us a great tour of some cool STAN fundamentals. Lets get started.
library(tidyverse) library(rstan) Our approach will be to tag a few fish, wait a while, then catch some more. The proportion of new fish with original tags will give us the information we need to make estimates about the larger population.</description>
    </item>
    
    <item>
      <title>Business Writing for Data Scientists</title>
      <link>/2020/03/14/business-writing-for-data-scientists/</link>
      <pubDate>Sat, 14 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/14/business-writing-for-data-scientists/</guid>
      <description>In this blog post we’ll focus on the task of writing. Emails, proposals, reports, and designs all convey our ideas. The way we dress up our ideas matter. Those who write poorly create barriers between themselves and their readers; those who write well connect with readers, open their minds, and achieve goals. Good writing gets ideas noticed. It gets them realized. So don’t be mislead: Writing well is a big deal.</description>
    </item>
    
    <item>
      <title>Intro to Bayes: Part 2</title>
      <link>/2020/03/08/intro-to-bayes-part-2/</link>
      <pubDate>Sun, 08 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/08/intro-to-bayes-part-2/</guid>
      <description>In this post, we&amp;rsquo;ll continue to use the coin flip example from part 1. Recall that we are interested in the posterior distribution of the parameter θ, which is the probability that a coin toss results in “heads”. Our prior distribution is an uninformative beta distribution with parameters 1 and 1. We also used a binomial likelihood function to estimate how representative a candidate value for θ is to have generated the data we have on hand - 3 observed heads out of 9 tosses.</description>
    </item>
    
    <item>
      <title>Embracing Uncertainty</title>
      <link>/2020/02/28/embracing-uncertainty/</link>
      <pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/28/embracing-uncertainty/</guid>
      <description>One of my favorite things about being a data scientist is running experiments. Often, data science teams will run a lot of experiments that aim to measure the same thing - Usually because we only have some short window to learn something or some other constraints. The purpose of most experiments is to recover an effect size for something we care about. When the experiment is over, we have typically measured this effect size with some uncertainty (like a standard error) about this measurement.</description>
    </item>
    
    <item>
      <title>Intro to Bayes: Part 1</title>
      <link>/2020/02/08/intro-to-bayes-part-1/</link>
      <pubDate>Sat, 08 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/08/intro-to-bayes-part-1/</guid>
      <description>In this blog post, I will give you a relatively nontechnical introduction to Bayesian statistics. Bayesian methods are getting more and more popular and there are many tools out there to apply it. In this example, we&amp;rsquo;ll use a computationally simple example to pin down concepts and jargon. In a follow-up post, I will introduce the basics of Markov chain Monte Carlo (MCMC) and the Metropolis–Hastings algorithm to solve more complex problems.</description>
    </item>
    
    <item>
      <title>Tea, Oxen, and Discrete Parameters</title>
      <link>/2020/02/01/tea-oxen-and-discrete-parameters/</link>
      <pubDate>Sat, 01 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/02/01/tea-oxen-and-discrete-parameters/</guid>
      <description>This is a retelling of Richard McElreath’s story about tea, oxen, and discrete parameters in Stan. In a Chinese village, children love tea but can only enjoy it after stabling their oxen. The problem is, children are liars sometimes and its up to us enforce the rules.
 You are the village enforcer of oxen customs… Each evening, you must determine which children have properly stabled their oxen. For many houses, you can see whether or not the ox is stabled.</description>
    </item>
    
    <item>
      <title>T-Test is not BEST</title>
      <link>/2019/12/31/t-test-is-not-best/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/31/t-test-is-not-best/</guid>
      <description>In this post, we explore the task of comparing groups of measurements. We’ve all heard about the T-Test, but Bayesian statistics offers an interesting alternative. Let’s talk about how “Bayesian estimation supersedes the t-test” (Kruschke, 2013).
Preliminaries In this post, we’ll encode Kruschke’s methods in STAN and use it to compare action move ratings versus comedy movie ratings. The code shown below is also available in python here along with the supporting CSV.</description>
    </item>
    
    <item>
      <title></title>
      <link>/1/01/01/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/1/01/01/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
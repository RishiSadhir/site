<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.62.1" />


<title>Tea, Oxen, and Discrete Parameters - Rishi Sadhir&#39;s personal blog</title>
<meta property="og:title" content="Tea, Oxen, and Discrete Parameters - Rishi Sadhir&#39;s personal blog">


  <link href='../../../../favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../../../../css/fonts.css" media="all">
<link rel="stylesheet" href="../../../../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../../../../" class="nav-logo">
    <img src="../../../../images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../../../../about/">About</a></li>
    
    <li><a href="https://github.com/RishiSadhir">GitHub</a></li>
    
    <li><a href="https://twitter.com/LordQuas3000">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">10 min read</span>
    

    <h1 class="article-title">Tea, Oxen, and Discrete Parameters</h1>

    
    <span class="article-date">2020-02-01</span>
    

    <div class="article-content">
      


<p>This is a retelling of Richard McElreath’s <a href="https://elevanth.org/blog/2018/01/29/algebra-and-missingness/">story</a> about tea, oxen, and discrete parameters in Stan. In a Chinese village, children love tea but can only enjoy it after stabling their oxen. The problem is, children are <em>liars</em> sometimes and its up to us enforce the rules!</p>
<blockquote>
<p>You are the village enforcer of oxen customs… Each evening, you must determine which children have properly stabled their oxen. For many houses, you can see whether or not the ox is stabled. But other houses have enclosed stables, so you cannot observe the ox without appearing to accuse that child of violating the rule. To do so and then discover a properly stabled ox would be embarrassing for everyone…</p>
</blockquote>
<p>So, there is a cost to revealing whether or not a child has stabled their oxen. We also want to know whether tea-drinkers or non-tea-drinkers are a better bet to blame. We need a model to help us reason about all this.</p>
<div id="preliminaries" class="section level1">
<h1>Preliminaries</h1>
<p>This post is targeted at folks who already have an understanding of bayesian statistics. Throughout, I’ll use <a href="https://mc-stan.org/">Stan</a> as my primary modeling tool. There is a lot you can do to make your Stan code more efficient but this post shoots for interpretability. To learn more about Bayesian Statistics and Stan, I highly recommend Richard’s book <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a> and the wonderful documentation on the Stan website.</p>
<pre class="r"><code># Interface with Stan
library(rstan)
# Tools for working with posteriors
library(tidybayes)
# Stan options
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)</code></pre>
<p>I use R and the tidyverse to wrangle data and dispatch analyses to Stan. You can learn more about the tidyverse through Hadley Wikham’s free online book <a href="https://r4ds.had.co.nz/">R4DS</a>. By the way, Stan is an external library that works just fine with Python too… Feel free to use whatever you’re most comfortable with.</p>
<pre class="r"><code># Tidyverse for wrangling and plotting
library(tidyverse)
library(patchwork)
# Set up plot theme
theme_minimal() %+replace%
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        plot.background = element_rect(color = &quot;transparent&quot;, fill = &quot;grey90&quot;)) %&gt;% 
  theme_set()</code></pre>
<p>With that out of the way, lets return to the problem at hand.</p>
</div>
<div id="building-a-model-for-oxen-and-tea" class="section level1">
<h1>Building a model for oxen and tea</h1>
<p>To create our model, lets think carefully about about this Chinese village and build a mini mathematical replica of it. As the enforcer, we walk around the village and observe children drinking tea or not drinking tea. We also observe each child’s stall… but only some of them are uncovered. Should we run the risk of checking on some covered stables?</p>
<p>Lets start by calling out some notation. We define three parameters below. Learning the values of these parameters will help us in our decision making process.</p>
<ul>
<li>Let <span class="math inline">\(p_{drink} = Pr(Tea | Ox)\)</span>. This is the probability of a child drinking tea given they have stabled their ox. Zero is a plausible value for this probability as not every child may want to drink tea.</li>
<li>Let <span class="math inline">\(p_{cheat} = Pr(Tea | No\_Ox)\)</span> be the probability of a child drinking tea given they have <em>not</em> stabled their ox.</li>
<li>Let <span class="math inline">\(s_i\)</span> indicate whether or not a child has stabled their ox.</li>
</ul>
<p>With these building blocks, we can build our little math world. <span class="math inline">\(s_i\)</span> and <span class="math inline">\(Tea_i\)</span> are given data for each child <span class="math inline">\(i\)</span>. Recall that some stables are covered and thus in unobserved states.
<span class="math display">\[
\begin{align}
&amp;Tea_i \sim Bernoulli(\pi_i) &amp; \text{[Child i drinks tea with probability } \pi_i] \\
&amp;\pi_i = s_ip_{drink} + (1 - s_i)p_{cheat} &amp; [\pi_i \text{ is influenced by stabling ]}\\
&amp;s_i \sim Bernoulli(\sigma) &amp; [\sigma \text{ is the proportion of children who stable]}\\
&amp;p_{drink} \sim Beta(2, 2) &amp; \text{[Mildly regularizing priors]}\\
&amp;p_{cheat} \sim Beta(2, 2) \\
&amp;\sigma \sim Beta(2, 2)
\end{align}
\]</span></p>
<p>Complexity arises from the fact that we don’t observe all stables, <span class="math inline">\(s_i\)</span>. We deal with this in line 3. We lets the “stabling proportion” be a parameter in our model and let Stan average over our ignorance. This means <span class="math inline">\(\sigma\)</span> will just represent the proportion of children who stable. Its good practice to <em>not</em> throw out the observations where we can’t see inside the stable. This method of Bayesian imputation fills lets us propagate our uncertainty forward to our parameter estimates. Finally, We chose mildly regularizing priors in the last 3 lines for the probability of cheating, drinking, and proportion of children who stable.</p>
<pre class="r"><code>tdf &lt;- tibble(x = rbeta(1e4, 2, 2)) 
interval &lt;- HDInterval::hdi(tdf$x)

tdf %&gt;% 
  ggplot(aes(x)) +
    stat_density(geom = &quot;line&quot;, size = 1, color = &quot;steelblue&quot;) +
    geom_segment(aes(y = 0, yend = 0, 
                     x = interval[1], xend = interval[2]),
                 color = &quot;steelblue&quot;) +
    geom_point(aes(y = 0, x = mean(tdf$x)),
               shape = 1, size = 3, color = &quot;steelblue&quot;) +
    scale_y_continuous(NULL, NULL) +
    scale_x_continuous(breaks = seq(from = 0, to = 1, by = .1)) +
    theme(#axis.line.x.bottom = element_line(),
          panel.grid.major.x = element_line(color = &quot;white&quot;, size = 1)) +
    labs(x = &quot;Probability&quot;,
         title = &quot;Mildly regularizing prior&quot;,
         subtitle = &quot;Shown with 95% credibility about the mean&quot;,
         caption = &quot;Beta(2, 2)\nUsed for Pr(cheat), Pr(drink), and Pr(stable)\n&quot;)</code></pre>
<p><img src="../../../../post/Bayesian_tea_and_oxen_files/figure-html/unnamed-chunk-3-1.png" width="576" angle=90 style="display: block; margin: auto;" /></p>
<p>So how will we use this model once we’ve found it’s parameters? We’ll calculate new parameters that measure exactly what we need to help with our enforcing decisions. First of all, we can calculate the probability of drinking tea for any child. Below, we use the law of <a href="https://brilliant.org/wiki/law-of-iterated-expectation/">iterated expectations</a> to break out the probability into known chunks. Thats just a fancy way to say you can calculate the probability of an event by looking at every “sub” event contained within it. When we see children drinking tea, they’ve either stabled their ox or they have not.</p>
<p><span class="math display">\[
\begin{align}
Pr(Tea) &amp;= Pr(Tea|Ox)Pr(Ox) + Pr(Tea | No\_Ox)Pr(No\_Ox) \\
&amp;= \sigma p_{drink} + (1 - \sigma)p_{cheat}
\end{align}
\]</span></p>
<p>Leveraging Bayes formula, we can use this to figure out the probability a child is lying. This tells us the probability of a tea drinking child having not stabled their ox. As enforcers, this will be important to our decision making process.</p>
<p><span class="math display">\[
\begin{align}
Pr(No\_ox | tea) &amp;= \frac{Pr(Tea | No\_Ox)Pr(No\_Ox)}{Pr(Tea)} \\
&amp;= \frac{(1 - \sigma) p_{cheat}}{\sigma p_{drink} + (1 - \sigma) p_{cheat}}
\end{align}
\]</span></p>
<p>And finally, we can calculate the probability of a child not stabling given they are not drinking tea.
<span class="math display">\[
\begin{align}
Pr(No\_ox | No\_tea) &amp;= \frac{Pr(No\_Tea | No\_Ox)Pr(No\_Ox)}{Pr(No\_tea)} \\
&amp;= \frac{(1 - \sigma) (1-p_{cheat})}{\sigma (1-p_{drink}) + (1 - \sigma) (1-p_{cheat})}
\end{align}
\]</span></p>
<p>The last two probability statements will be essential to our jobs. They quantify the likelihood of a child not stabling their ox when we observe tea drinking and when we don’t observe tea drinking. If we have an understanding of the “cost” of our own embarrasment, we can bake these in to a utility function that would help us understand whether or not we should pull that childs card.</p>
<p>Okay, now that we’ve figured out the math, lets take it to the data!</p>
</div>
<div id="application" class="section level1">
<h1>Application</h1>
<p>In this section, we’ll simulate data with made up parameters then use Stan to recover them. This is something you should get use to as a Baysian practitioner. It represents the bigger picture of Bayesian data analysis. We specify the data generating process by simulating the data we are expecting to see. We then encode that simulation code in Stan but leave our unknowns as parameters. Finally, we feed Stan real data and let it search around for the paramers that best describe it.</p>
<div id="simulation" class="section level3">
<h3>Simulation</h3>
<p>First we’ll simulate the data. Note how our model, <span class="math inline">\(pi_i = s_ip_{drink} + (1 - s_i)p_{cheat}\)</span>, is represented directy in this code block.</p>
<pre class="r"><code># The seed makes our analysis repeatable
set.seed(1)
# There are 100 children in the village
N_children &lt;- 100
# 44 stables wre unobservable
idx_covered_stables &lt;- sample(1:N_children, size = 22)
# These parameters are unknown in the real data
## 75 Percent of children stable their ox
p_stabled &lt;- .75
## After stabling ox, 100% of children drink tea
p_drink &lt;- 1
## Unstabling children cheat 50% of the time
p_cheat &lt;- .5

# Simulate!
set.seed(1)
stabled &lt;- rbinom(N_children, size=1, prob=p_stabled)
stabled_obs &lt;- stabled
stabled_obs[idx_covered_stables] &lt;- -1
tea &lt;- rbinom(N_children, size=1, prob=stabled*p_drink + (1-stabled)*p_cheat)

df &lt;- tibble(stabled, stabled_obs, tea)
head(df)</code></pre>
<pre><code>## # A tibble: 6 x 3
##   stabled stabled_obs   tea
##     &lt;int&gt;       &lt;dbl&gt; &lt;int&gt;
## 1       1          -1     1
## 2       1           1     1
## 3       1           1     1
## 4       0           0     1
## 5       1           1     1
## 6       0           0     0</code></pre>
</div>
<div id="stan" class="section level3">
<h3>Stan</h3>
<p>Now we encode our model in stan. The likelihood portion of our stan model is really the same as the simulation code above. <code>log_mix</code> might be a bit confusing, but its just doing the same thing as the mixture in our simulation above <span class="math inline">\(pi_i = s_ip_{drink} + (1 - s_i)p_{cheat}\)</span>.</p>
<pre class="stan"><code>data {
  // Number of children in city
  int n;
  // Stable status (-1 is unobserved)
  int stabled_obs[n];
  // Observed tea drinking
  int tea[n];
}
parameters{
  real&lt;lower=0,upper=1&gt; p_cheat;
  real&lt;lower=0,upper=1&gt; p_drink;
  real&lt;lower=0&gt; sigma;
}
model{
  // priors
  p_cheat ~ beta(2,2);
  p_drink ~ beta(2,2);
  sigma   ~ beta(2,2);

  // probability of tea
  for (i in 1:n) {
    if (stabled_obs[i] == -1) {
      // ox unobserved
      target += log_mix( 
                  sigma , 
                  bernoulli_lpmf(tea[i] | p_drink) , 
                  bernoulli_lpmf(tea[i] | p_cheat));
    } else {
      // ox observed
      tea[i] ~ bernoulli(stabled_obs[i]*p_drink + (1-stabled_obs[i])*p_cheat);
      stabled_obs[i] ~ bernoulli(sigma);
    }
  }
}</code></pre>
<p>Now, lets take our model and condition it on our simulated data to see if we recovered our parameters. Our 95% credible intervals contain the recovered parameter values.</p>
<pre class="r"><code>stan_fit &lt;- sampling(stan_model, compose_data(df),
         chains = 4, cores = 4, refresh = 0,
         control = list(adapt_delta = .99))
print(stan_fit, probs = c(.05, .95))</code></pre>
<pre><code>## Inference for Stan model: 22811e608fb0a2aa37c70866058b8526.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##           mean se_mean   sd     5%    95% n_eff Rhat
## p_cheat   0.45    0.00 0.09   0.29   0.61  2241    1
## p_drink   0.97    0.00 0.02   0.92   0.99  3200    1
## sigma     0.73    0.00 0.05   0.65   0.80  2388    1
## lp__    -83.71    0.04 1.26 -86.18 -82.36  1160    1
## 
## Samples were drawn using NUTS(diag_e) at Sat Feb  8 20:39:17 2020.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>An awesome benefit of Bayesian data analysis is that we get to work with a posterior afterwards. In the next section, we’ll take advantage of this by using this rich understanding of our parameters in to our decision making process.</p>
<pre class="r"><code>df_posterior &lt;- gather_draws(stan_fit, p_cheat, p_drink, sigma)

p1 &lt;- df_posterior %&gt;% 
  ggplot(aes(.value)) +
    stat_density(geom = &quot;line&quot;, color = &quot;steelblue&quot;) +
    scale_y_continuous(NULL, NULL) +
    scale_x_continuous(breaks = seq(from = 0, to = 1, by = .1)) +
    facet_wrap(~ .variable, ncol = 1, scales = &quot;free_y&quot;) +
    labs(x = &quot;Posterior Value&quot;,
         title = &quot;Parameter fit&quot;)

p2 &lt;- df_posterior %&gt;% 
  mutate(chain = as.factor(.chain)) %&gt;% 
  ggplot(aes(y = .value, x = .iteration, color = chain)) +
    geom_line(alpha = .7) +
    facet_wrap(~ .variable, ncol = 1, scales = &quot;free_y&quot;) +
    scale_color_viridis_d() +
    theme(axis.ticks.y = element_line()) +
    labs(y = &quot;&quot;, x = &quot;Iteration&quot;,
         title = &quot;Parameter Exploration&quot;)

p1 + p2</code></pre>
<p><img src="../../../../post/Bayesian_tea_and_oxen_files/figure-html/unnamed-chunk-7-1.png" width="768" angle=90 style="display: block; margin: auto;" /></p>
</div>
<div id="emperics" class="section level3">
<h3>Emperics</h3>
<pre class="r"><code>df_posterior &lt;- spread_draws(stan_fit, p_cheat, p_drink, sigma)

df_posterior &lt;- df_posterior %&gt;% 
  mutate(&quot;Pr(No_Ox|tea)&quot; = 
           ((1 - sigma)*p_cheat) /
           (sigma*p_drink + (1-sigma)*p_cheat))
p1 &lt;- df_posterior %&gt;% 
  ggplot(aes(`Pr(No_Ox|tea)`)) +
  stat_density(geom = &quot;line&quot;, color = &quot;steelblue&quot;) +
  scale_x_continuous(breaks = seq(from = 0, to = 1, by = .1),
                     limits = c(0, 1)) +
  scale_y_continuous(NULL, NULL) +
  theme(axis.line.x.bottom = element_line()) +
  ggtitle(&quot;Posterior probabilities of bad animal husbandry&quot;)

df_posterior &lt;- df_posterior %&gt;% 
  mutate(&quot;Pr(No_Ox|no_tea)&quot; = 
           ((1 - sigma)*(1-p_cheat)) /
           (sigma*(1-p_drink) + (1-sigma)*(1-p_cheat)))
p2 &lt;- df_posterior %&gt;% 
  ggplot(aes(`Pr(No_Ox|no_tea)`)) +
  stat_density(geom = &quot;line&quot;, color = &quot;steelblue&quot;) +
  scale_x_continuous(breaks = seq(from = 0, to = 1, by = .1),
                     limits = c(0, 1)) +
  scale_y_continuous(NULL, NULL) +
  theme(axis.line.x.bottom = element_line())

p1 / p2</code></pre>
<p><img src="../../../../post/Bayesian_tea_and_oxen_files/figure-html/unnamed-chunk-8-1.png" width="672" angle=90 style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="conclusion" class="section level1">
<h1>Conclusion</h1>
<p>Alright, we’ve gone through a journey from a small Chinese village all the way to mixture models. So should we check in on the children’s stables when they aren’t drinking tea? Our next step would be to definie a utility function that describes how much we value embarrassment vs. catching rule breakers. We can ultimately match those up with our posterior distributions and utilize Bayesian decision theory to do whats best. This is a topic we’ll work on in another post.</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../../../../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../../../../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../../../../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
  </body>
</html>


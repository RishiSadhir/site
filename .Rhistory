reduce(rbind) %>%
as_tibble
names(x_train)
colnames(x_train)
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train))
length(cvfit$lambda)
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda)
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
ggplot(aes(x = log(lambda), y = Hits)) +
geom_point()
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
ggplot(aes(x = log(lambda), y = Hits)) +
geom_hline(yintercept = 0)
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
ggplot(aes(x = log(lambda), y = Hits)) +
geom_hline(yintercept = 0) +
geom_point()
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda)
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = vars(-lambda))
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda),
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = Hits)) +
geom_hline(yintercept = 0) +
geom_point()
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda)
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, shape = name)) +
geom_hline(yintercept = 0) +
geom_point()
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, linetype = name)) +
geom_hline(yintercept = 0) +
geom_line()
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, linetype = name, color = name)) +
geom_hline(yintercept = 0) +
geom_line()
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, linetype = name, color = name)) +
geom_hline(yintercept = 0) +
geom_line()
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, linetype = name, color = name)) +
geom_hline(yintercept = 0) +
geom_line()
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, linetype = name, color = name)) +
geom_hline(yintercept = 0) +
geom_line()
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, linetype = name, color = name)) +
geom_hline(yintercept = 0) +
geom_line() +
scale_color_viridis_d()
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, linetype = name, color = name)) +
geom_hline(yintercept = 0) +
geom_line() +
scale_color_viridis_d()  +
xlab(expression(log(lambda))) + ylab("Coefficient Path")
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, linetype = name, color = name)) +
geom_hline(yintercept = 0) +
geom_line() +
scale_color_viridis_d()  +
xlab(expression(log(lambda))) + ylab("Coefficient Path") +
ggtitle("As lambda  increases, so does the extent to which coefficients shrink towards 0")
# Interface with Stan
library(rstan)
# Helpers for working with Stan
library(tidybayes)
# Stan options
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
# Tidyverse for wrangling and plotting
library(tidyverse)
# Patchwork stitches plots together
library(patchwork)
# My plot theme
theme_set(theme_gray() +
theme(axis.text.y  = element_text(hjust = 0),
panel.background = element_rect(color = "transparent",
fill = "#F6F4EE"),
panel.grid = element_blank(),
strip.background = element_rect(color = "transparent", fill = "#EFECE2"),
text = element_text(family = "Helvetica")))
library(recipes)
library(rsample)
set.seed(18060)
df_hitters <- as_tibble(ISLR::Hitters)
# Train/Test split
data_split <- initial_split(df_hitters[!is.na(df_hitters$Salary),], strata = "Salary", na.rm = TRUE)
hitters_train <- training(data_split)
hitters_test <- testing(data_split)
# Preprocess
hyp <- Salary ~ AtBat + Hits + HmRun + Runs +
RBI + Walks + Years + CAtBat + CHits +
CHmRun + CRuns + CRBI + CWalks + PutOuts +
Assists + Errors
basic_rec <- recipe(hyp, data = hitters_train) %>%
# remove variables with zero variance
step_zv(all_predictors()) %>%
# center and scale numeric variables
step_center(all_numeric()) %>% step_scale(all_numeric()) %>%
# ensure there is an intercept
step_intercept()
prepped <- basic_rec %>% prep(training=hitters_train)
# train matrices
x_train <- prepped %>%
juice(all_predictors(), composition='matrix')
y_train <- prepped %>%
juice(all_outcomes())
# Plot processed covarites
as_tibble(x_train[,-1]) %>%
gather(var, val) %>%
ggplot(aes(val)) +
stat_density(geom = "line", size = 1, color = "#620E62") +
scale_y_continuous(NULL, NULL) +
scale_x_continuous(NULL, breaks = -3:6) +
facet_wrap(~ var) +
ggtitle("Center and scale all covariates")
suppressPackageStartupMessages(library(glmnet))
# glmnet automatically adds a coefficient, so drop the one we made
# We already standardized in our preprocessing
cvfit = cv.glmnet(x=x_train[,-1], y=y_train$Salary, family='gaussian',
alpha=1, standardize=FALSE)
# Plot the complete lambda path
tibble(lambda = cvfit$lambda,
mse = cvfit$cvm,
sd = cvfit$cvsd,
up = cvfit$cvup,
lo = cvfit$cvlo) %>%
ggplot(aes(x = log(lambda), y = mse, ymin = lo, ymax = up)) +
geom_vline(xintercept = log(cvfit$lambda.min), color = "#84C6D1", linetype = 2) +
geom_vline(xintercept = log(cvfit$lambda.1se), color = "#84C6D1", linetype = 2) +
geom_point(color = "#01596E") +
geom_linerange(color = "#01596E") +
scale_x_continuous(expression(log(lambda)), breaks = -8:0) +
annotate("text", x = log(cvfit$lambda.min), y = 1, label = "Min") +
annotate("text", x = log(cvfit$lambda.1se), y = 1, label = "1se") +
ggtitle("Glmnet efficiently computes the entire lambda path") +
ylab("Mean-Squared Error")
coef(cvfit, s = cvfit$lambda.min)
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, linetype = name, color = name)) +
geom_hline(yintercept = 0) +
geom_line() +
scale_color_viridis_d()  +
xlab(expression(log(lambda))) + ylab("Coefficient Path") +
ggtitle("As lambda  increases, so does the extent to which coefficients shrink towards 0")
lm(x=x_train, y=y_train$Salary)
?lm.fit
lm.fit(x=x_train, y=y_train$Salary)
lmfit <- lm.fit(x=x_train, y=y_train$Salary)
coef(lmfit)
plot(coef(lmfit))
tibble(s = 1:10) %>%
mutate(samples = list(rmutil::rlaplace(1e4, 0, s))) %>%
unnest(cols = c(samples)) %>%
mutate(s = as.factor(s)) %>%
ggplot(aes(samples, color = s)) +
stat_density(geom = "line") +
scale_x_continuous(limits = c(-50, 50)) +
scale_color_viridis_d() +
ggtitle("We can control the amount of regularization with the s parameter") +
theme(legend.background = element_rect(color = "black",fill = "transparent"),
legend.key = element_blank(),
legend.position = c(.975,.55))
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, color = name)) +
geom_hline(yintercept = 0) +
geom_line() +
xlab(expression(log(lambda))) + ylab("Coefficient Path") +
ggtitle("As lambda  increases, so does the extent to which coefficients shrink towards 0")
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, color = name)) +
geom_hline(yintercept = 0, size = 2) +
geom_line(alpha = .8) +
xlab(expression(log(lambda))) + ylab("Coefficient Path") +
ggtitle("As lambda  increases, so does the extent to which coefficients shrink towards 0")
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, color = name)) +
geom_hline(yintercept = 0, size = 1) +
geom_line(alpha = .8) +
xlab(expression(log(lambda))) + ylab("Coefficient Path") +
ggtitle("As lambda  increases, so does the extent to which coefficients shrink towards 0")
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, color = name)) +
geom_hline(yintercept = 0, size = 1, linetype = 2) +
geom_line(alpha = .8) +
xlab(expression(log(lambda))) + ylab("Coefficient Path") +
ggtitle("As lambda  increases, so does the extent to which coefficients shrink towards 0")
matrix(c(6, -1, 2, 3))
matrix(c(6, -1, 2, 3), nrow - 2)
matrix(c(6, -1, 2, 3), nrow = 2)
matrix(c(6, -1, 2, 3), ncol = 2)
matrix(c(6, 2, -1, 3), ncol = 2)
T = matrix(c(6, 2, -1, 3), ncol = 2)
C = matrix(c(1, 1, 1, 2), ncol = 2)
C
inv(T)
solve(C)
solve(T)
solve(C) %*% T %*% C
solve(C) %*% T %*% C
T = matrix(c(6, 2, -1, 3), ncol = 2)
C = matrix(c(1, 1, 1, 2), ncol = 2)
solve(C) %*% T %*% C
?solve
T = matrix(c(2, 7, 0, -1), nrow = 2, byrow = TRUE)
T
T = matrix(c(2, 7, 0, -1), nrow = 2, byrow = TRUE)
C =  matrix(c(7, 1, -3, 0), nrow = 2, byrow = TRUE)
solve(C) %*% T %*% C
T
C
T = matrix(c(2, 7, 0, -1), nrow = 2, byrow = TRUE)
C =  matrix(c(7, 1, -3, 0), nrow = 2, byrow = TRUE)
solve(C) %*% T %*% C
T = matrix(c(1, 0, 2, -1), nrow = 2, byrow = TRUE)
C =  matrix(c(1, 0, 1, 1), nrow = 2, byrow = TRUE)
solve(C) %*% T %*% C
D = matrix(c(5, 0, 0, 4), nrow = 2, byrow = TRUE)
D^3
C =  matrix(c(1, 1, 1, 2), nrow = 2, byrow = TRUE)
D = matrix(c(5, 0, 0, 4), nrow = 2, byrow = TRUE)
solve(C)
C =  matrix(c(1, 1, 1, 2), nrow = 2, byrow = TRUE)
D = matrix(c(5, 0, 0, 4), nrow = 2, byrow = TRUE)
solve(C) %*% D^3 %*% C
C =  matrix(c(1, 1, 1, 2), nrow = 2, byrow = TRUE)
D = matrix(c(5, 0, 0, 4), nrow = 2, byrow = TRUE)
C %*% D^3 %*% solve(C)
C =  matrix(c(1, 1, 1, 2), nrow = 2, byrow = TRUE)
D = matrix(c(5, 0, 0, 4), nrow = 2, byrow = TRUE)
C %*% D^3 %*% solve(C)
C =  matrix(c(7, 1, -3, 0), nrow = 2, byrow = TRUE)
D = matrix(c(-1, 0, 0, 2), nrow = 2, byrow = TRUE)
solve(C)
C =  matrix(c(7, 1, -3, 0), nrow = 2, byrow = TRUE)
D = matrix(c(-1, 0, 0, 2), nrow = 2, byrow = TRUE)
C %*% D^3 %*% solve(C)
C =  matrix(c(1, 0, 1, 1), nrow = 2, byrow = TRUE)
D = matrix(c(-1, 0, 0, -1), nrow = 2, byrow = TRUE)
solve(C)
C =  matrix(c(1, 0, 1, 1), nrow = 2, byrow = TRUE)
D = matrix(c(-1, 0, 0, -1), nrow = 2, byrow = TRUE)
C %*% D^3 %*% solve(C)
C =  matrix(c(1, 0, 1, 1), nrow = 2, byrow = TRUE)
D = matrix(c(-1, 0, 0, -1), nrow = 2, byrow = TRUE)
C %*% D^3 %*% solve(C)
C =  matrix(c(1, 0, 2, -1), nrow = 2, byrow = TRUE)
D = matrix(c(-1, 0, 0, -1), nrow = 2, byrow = TRUE)
C %*% D^3 %*% solve(C)
C =  matrix(c(1, 0, 2, -1), nrow = 2, byrow = TRUE)
D = matrix(c(-1, 0, 0, -1), nrow = 2, byrow = TRUE)
C %*% D^3 %*% solve(C)
C =  matrix(c(1, 0, 2, -1), nrow = 2, byrow = TRUE)
D = matrix(c(-1, 0, 0, -1), nrow = 2, byrow = TRUE)
C %*% D^5 %*% solve(C)
C =  matrix(c(1, 0, 2, -9), nrow = 2, byrow = TRUE)
D = matrix(c(-1, 0, 0, -1), nrow = 2, byrow = TRUE)
C %*% D^5 %*% solve(C)
C =  matrix(c(1, 0, 2, -1), nrow = 2, byrow = TRUE)
D = matrix(c(-1, 0, 0, -1), nrow = 2, byrow = TRUE)
C %*% D^5 %*% solve(C)
C %*% D^5 %*% solve(C)
C =  matrix(c(1, 0, 2, -1), nrow = 2, byrow = TRUE)
C
C =  matrix(c(1, 0, 1, 1), nrow = 2, byrow = TRUE)
D = matrix(c(-1, 0, 0, -1), nrow = 2, byrow = TRUE)
C %*% D^5 %*% solve(C)
C
\begin{align}
CVR_{ij} &= Binomial(p_i) \\
logit(p_i) &= \alpha_{sku[i]} + poly(sortposition)
\end{align}
$$
\begin{align}
CVR_{ij} &= Binomial(p_i) \\
logit(p_i) &= \alpha_{sku[i]} + poly(sortposition)
\end{align}
$$
\begin{align}
CVR_{ij} &= Binomial(p_i) \\
logit(p_i) &= \gamma_{sku[i]} + poly(sortposition)
\end{align}
# Interface with Stan
library(rstan)
# Helpers for working with Stan
library(tidybayes)
# Stan options
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
# Tidyverse for wrangling and plotting
library(tidyverse)
# Patchwork stitches plots together
library(patchwork)
# My plot theme
theme_set(theme_gray() +
theme(axis.text.y  = element_text(hjust = 0),
panel.background = element_rect(color = "transparent",
fill = "#F6F4EE"),
panel.grid = element_blank(),
strip.background = element_rect(color = "transparent", fill = "#EFECE2"),
text = element_text(family = "Helvetica")))
library(recipes)
library(rsample)
set.seed(18060)
df_hitters <- as_tibble(ISLR::Hitters)
# Train/Test split
data_split <- initial_split(df_hitters[!is.na(df_hitters$Salary),], strata = "Salary", na.rm = TRUE)
hitters_train <- training(data_split)
hitters_test <- testing(data_split)
# Preprocess
hyp <- Salary ~ AtBat + Hits + HmRun + Runs +
RBI + Walks + Years + CAtBat + CHits +
CHmRun + CRuns + CRBI + CWalks + PutOuts +
Assists + Errors
basic_rec <- recipe(hyp, data = hitters_train) %>%
# remove variables with zero variance
step_zv(all_predictors()) %>%
# center and scale numeric variables
step_center(all_numeric()) %>% step_scale(all_numeric()) %>%
# ensure there is an intercept
step_intercept()
prepped <- basic_rec %>% prep(training=hitters_train)
# train matrices
x_train <- prepped %>%
juice(all_predictors(), composition='matrix')
y_train <- prepped %>%
juice(all_outcomes())
# Plot processed covarites
as_tibble(x_train[,-1]) %>%
gather(var, val) %>%
ggplot(aes(val)) +
stat_density(geom = "line", size = 1, color = "#620E62") +
scale_y_continuous(NULL, NULL) +
scale_x_continuous(NULL, breaks = -3:6) +
facet_wrap(~ var) +
ggtitle("Center and scale all covariates")
suppressPackageStartupMessages(library(glmnet))
# glmnet automatically adds a coefficient, so drop the one we made
# We already standardized in our preprocessing
cvfit = cv.glmnet(x=x_train[,-1], y=y_train$Salary, family='gaussian',
alpha=1, standardize=FALSE)
# Plot the complete lambda path
tibble(lambda = cvfit$lambda,
mse = cvfit$cvm,
sd = cvfit$cvsd,
up = cvfit$cvup,
lo = cvfit$cvlo) %>%
ggplot(aes(x = log(lambda), y = mse, ymin = lo, ymax = up)) +
geom_vline(xintercept = log(cvfit$lambda.min), color = "#84C6D1", linetype = 2) +
geom_vline(xintercept = log(cvfit$lambda.1se), color = "#84C6D1", linetype = 2) +
geom_point(color = "#01596E") +
geom_linerange(color = "#01596E") +
scale_x_continuous(expression(log(lambda)), breaks = -8:0) +
annotate("text", x = log(cvfit$lambda.min), y = 1, label = "Min") +
annotate("text", x = log(cvfit$lambda.1se), y = 1, label = "1se") +
ggtitle("Glmnet efficiently computes the entire lambda path") +
ylab("Mean-Squared Error")
coef(cvfit, s = cvfit$lambda.min)
map(cvfit$lambda, ~as.vector(coef(cvfit, s = .x))) %>%
reduce(rbind) %>%
as_tibble %>%
set_names(colnames(x_train)) %>%
mutate(lambda = cvfit$lambda) %>%
pivot_longer(cols = -lambda) %>%
ggplot(aes(x = log(lambda), y = value, color = name)) +
geom_hline(yintercept = 0, size = 1, linetype = 2) +
geom_line(alpha = .8) +
xlab(expression(log(lambda))) + ylab("Coefficient Path") +
ggtitle("As lambda  increases, so does the extent to which coefficients shrink towards 0")
tibble(s = 1:10) %>%
mutate(samples = list(rmutil::rlaplace(1e4, 0, s))) %>%
unnest(cols = c(samples)) %>%
mutate(s = as.factor(s)) %>%
ggplot(aes(samples, color = s)) +
stat_density(geom = "line") +
scale_x_continuous(limits = c(-50, 50)) +
scale_color_viridis_d() +
ggtitle("We can control the amount of regularization with the s parameter") +
theme(legend.background = element_rect(color = "black",fill = "transparent"),
legend.key = element_blank(),
legend.position = c(.975,.55))
setwd("~/Documents/personal/site")
blogdown::serve_site()
install.packages(
"blogdown"
)
blogdown::install_hugo()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::serve_site()
blogdown::stop_server()
blogdown::stop_server()
